# Netflix Prize Data Warehouse - ETL Pipeline

## üìã Table of Contents

- [Overview](#overview)
- [Features](#features)
- [Architecture](#architecture)
- [Prerequisites](#prerequisites)
- [Installation](#installation)
- [Configuration](#configuration)
- [Usage](#usage)
- [Project Structure](#project-structure)
- [Database Schema](#database-schema)
- [Troubleshooting](#troubleshooting)

---

## üì¶ Overview

This project implements a **production-grade ETL (Extract-Transform-Load) pipeline** for the Netflix Prize dataset using **Apache Spark** (PySpark) and **PostgreSQL**. It processes 100M+ movie ratings from 480K customers across 17K titles spanning 1998-2005, transforming raw data into a normalized Star Schema data warehouse optimized for analytical queries.

### Key Metrics

| Metric                   | Value                              |
| ------------------------ | ---------------------------------- |
| **Total Ratings**        | 100M+                              |
| **Unique Customers**     | ~480K                              |
| **Unique Movies**        | ~17K                               |
| **Date Range**           | Oct 1998 - Dec 2005                |
| **Rating Scale**         | 1-5 (integer)                      |
| **Processing Framework** | Apache Spark 3.4+                  |
| **Target Database**      | Azure PostgreSQL (Flexible Server) |

---

## ‚ú® Features

### üöÄ Performance & Scalability

- **Distributed Processing**: Leverages PySpark for parallel computation across multiple CPU cores
- **Optimized Partitioning**: 200+ shuffle partitions for efficient data distribution
- **Memory Management**: Automatic memory optimization with minimal garbage collection
- **JDBC Batching**: 10,000-record batches for efficient database writes
- **Lazy Evaluation**: Spark's execution plan optimization for minimal unnecessary computation

### üèóÔ∏è Data Quality

- **Comprehensive Validation**: Schema enforcement at multiple stages
- **Surrogate Keys**: Independent from source data for data warehouse standards
- **Deduplication**: Automatic handling of duplicate records
- **Type Safety**: Strong type checking with PySpark DataFrames
- **Idempotent Design**: Safe to re-run without data corruption

### üìä Data Warehouse Design

- **Star Schema**: Optimized for OLAP (Online Analytical Processing) queries
- **1 Fact Table**: `fact_ratings` containing 100M+ transactions
- **3 Dimension Tables**: `dim_customer`, `dim_movie`, `dim_date` for analytics
- **Strategic Indexing**: Composite indexes for common query patterns
- **Foreign Key Constraints**: Referential integrity enforcement

### üîç Monitoring & Logging

- **Dual Output**: Console + file logging (`etl_pipeline_spark.log`)
- **Real-time Progress**: Detailed status updates during execution
- **Error Tracking**: Comprehensive error messages with recovery guidance
- **Execution Timing**: Stage-by-stage performance metrics

---

## üèõÔ∏è Architecture

### Data Flow

```
Raw Data Files (text/CSV)
        ‚Üì
    PySpark ETL
        ‚Üì
  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
  ‚îÇ   Transform     ‚îÇ
  ‚îÇ  - Parse       ‚îÇ
  ‚îÇ  - Normalize   ‚îÇ
  ‚îÇ  - Deduplicate ‚îÇ
  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚Üì
  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
  ‚îÇ  Load to DB     ‚îÇ
  ‚îÇ  - Dimensions   ‚îÇ
  ‚îÇ  - Facts        ‚îÇ
  ‚îÇ  - Aggregates   ‚îÇ
  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚Üì
PostgreSQL Data Warehouse
```

### Component Architecture

```
SparkSessionManager
    ‚îú‚îÄ‚îÄ Credential Management
    ‚îú‚îÄ‚îÄ JDBC Configuration
    ‚îî‚îÄ‚îÄ Session Lifecycle

ETL Pipeline Stages
    ‚îú‚îÄ‚îÄ Stage 1: Date Dimension
    ‚îú‚îÄ‚îÄ Stage 2: Movie Dimension
    ‚îú‚îÄ‚îÄ Stage 3: Customer Dimension
    ‚îú‚îÄ‚îÄ Stage 4: Fact Table
    ‚îî‚îÄ‚îÄ Stage 5: Post-Processing

Database Schema (netflix_dw)
    ‚îú‚îÄ‚îÄ fact_ratings (BIGSERIAL PK)
    ‚îú‚îÄ‚îÄ dim_date (INTEGER PK)
    ‚îú‚îÄ‚îÄ dim_movie (SERIAL PK)
    ‚îî‚îÄ‚îÄ dim_customer (SERIAL PK)
```

---

## üìã Prerequisites

### Software Requirements

- **Python**: 3.9 or higher
- **Java**: 11+ (required for Apache Spark)
- **PostgreSQL**: 12+ or Azure Database for PostgreSQL (Flexible Server)
- **Apache Spark**: 3.4.0 or higher (included in PySpark distribution)

### Python Dependencies

```
pyspark>=3.4.0
python-dotenv>=1.0.0
pandas>=2.0.0
sqlalchemy>=2.0.0
psycopg2-binary>=2.9.0
```

### Database Driver

- **PostgreSQL JDBC Driver**: `postgresql-42.6.0.jar` (automatically handled by Spark)

---

## ‚öôÔ∏è Installation

### 1. Clone Repository

```bash
git clone <repository-url>
cd netflix-data-ingestion
```

### 2. Create Python Virtual Environment

**Windows (PowerShell):**

```powershell
python -m venv venv
.\venv\Scripts\Activate.ps1
```

**Linux/macOS:**

```bash
python3 -m venv venv
source venv/bin/activate
```

### 3. Install Dependencies

```bash
pip install -r requirements.txt
```

### 4. Download PostgreSQL JDBC Driver

Download [postgresql-42.6.0.jar](https://jdbc.postgresql.org/download.html) and place in project root:

```bash
# File location should be:
./postgresql-42.6.0.jar
```

### 5. Prepare Data Files

Ensure your data directory contains:

```
data/
‚îú‚îÄ‚îÄ movie_titles.csv          # 17K movies
‚îú‚îÄ‚îÄ combined_data_1.txt       # 25M ratings
‚îú‚îÄ‚îÄ combined_data_2.txt       # 25M ratings
‚îú‚îÄ‚îÄ combined_data_3.txt       # 25M ratings
‚îî‚îÄ‚îÄ combined_data_4.txt       # 25M ratings
```

---

## üîê Configuration

### Environment Variables

Create a `.env` file in project root (copy from `.env.example`):

```env
# PostgreSQL Connection
PGHOST=your-server.postgres.database.azure.com
PGPORT=5432
PGDATABASE=netflix_dw
PGUSER=adminuser@servername
PGPASSWORD=YourSecurePassword123!

# Optional: Spark Settings
SPARK_LOCAL_IP=127.0.0.1
HADOOP_HOME=./hadoop
```

### Environment Variable Guide

| Variable     | Purpose                       | Example                                |
| ------------ | ----------------------------- | -------------------------------------- |
| `PGHOST`     | Database server hostname      | `myserver.postgres.database.azure.com` |
| `PGPORT`     | Database port (default: 5432) | `5432`                                 |
| `PGDATABASE` | Database name                 | `netflix_dw`                           |
| `PGUSER`     | Database username             | `adminuser@servername`                 |
| `PGPASSWORD` | Database password             | `SecurePass123!`                       |

**‚ö†Ô∏è Security Note**: Never commit `.env` to version control. It's listed in `.gitignore`.

---

## üöÄ Usage

### Quick Start

```bash
# Activate virtual environment
.\venv\Scripts\Activate.ps1  # Windows
# or
source venv/bin/activate     # Linux/macOS

# Create database schema (first run only)
psql -h your-server -U adminuser -d postgres -f schema.sql

# Run ETL pipeline
python etl_pipeline_spark.py
```

### Expected Output

```
2025-12-15 10:30:45 - INFO - Initializing Spark session...
2025-12-15 10:30:52 - INFO - ====== NETFLIX DW ETL PIPELINE ======
2025-12-15 10:30:52 - INFO - Starting ETL process...
2025-12-15 10:31:00 - INFO - [STAGE 1/5] Loading Date Dimension...
2025-12-15 10:31:15 - INFO - Date dimension: 2,865 records loaded
2025-12-15 10:31:15 - INFO - [STAGE 2/5] Loading Movie Dimension...
2025-12-15 10:31:45 - INFO - Movie dimension: 17,770 records loaded
...
2025-12-15 10:45:30 - INFO - ‚úÖ ETL Pipeline Completed Successfully
2025-12-15 10:45:30 - INFO - Total execution time: 15 minutes
```

### Viewing Logs

Logs are written to both console and `etl_pipeline_spark.log`:

```bash
# Last 50 lines
Get-Content etl_pipeline_spark.log -Tail 50

# Full log
Get-Content etl_pipeline_spark.log
```

### Running with Custom Configuration

```bash
# Set environment-specific settings
$env:SPARK_LOCAL_IP = "127.0.0.1"

# Run pipeline
python etl_pipeline_spark.py
```

---

## üìÅ Project Structure

```
netflix-data-ingestion/
‚îú‚îÄ‚îÄ etl_pipeline_spark.py          # Main ETL pipeline (962 lines)
‚îú‚îÄ‚îÄ schema.sql                     # Database DDL script
‚îú‚îÄ‚îÄ requirements.txt               # Python dependencies
‚îú‚îÄ‚îÄ .env.example                   # Environment template
‚îú‚îÄ‚îÄ .env                          # Configuration (git-ignored)
‚îú‚îÄ‚îÄ .gitignore                    # Git ignore rules
‚îú‚îÄ‚îÄ README.md                     # This file
‚îÇ
‚îú‚îÄ‚îÄ data/                         # Data directory
‚îÇ   ‚îú‚îÄ‚îÄ movie_titles.csv         # 17K movie titles
‚îÇ   ‚îú‚îÄ‚îÄ combined_data_1.txt      # 25M ratings
‚îÇ   ‚îú‚îÄ‚îÄ combined_data_2.txt      # 25M ratings
‚îÇ   ‚îú‚îÄ‚îÄ combined_data_3.txt      # 25M ratings
‚îÇ   ‚îî‚îÄ‚îÄ combined_data_4.txt      # 25M ratings
‚îÇ
‚îú‚îÄ‚îÄ hadoop/                       # Hadoop binaries (Windows compatibility)
‚îÇ   ‚îî‚îÄ‚îÄ bin/                     # Executable files
‚îÇ
‚îú‚îÄ‚îÄ postgresql-42.6.0.jar        # PostgreSQL JDBC driver
‚îú‚îÄ‚îÄ etl_pipeline_spark.log       # Execution log (generated)
‚îî‚îÄ‚îÄ __pycache__/                 # Python cache (git-ignored)
```

---

## üóÑÔ∏è Database Schema

### Physical Data Model

#### **Fact Table: fact_ratings**

Central table containing all movie ratings transactions.

| Column         | Type      | Constraints       | Purpose            |
| -------------- | --------- | ----------------- | ------------------ |
| `rating_key`   | BIGSERIAL | PRIMARY KEY       | Unique identifier  |
| `customer_key` | INTEGER   | FK ‚Üí dim_customer | Customer reference |
| `movie_key`    | INTEGER   | FK ‚Üí dim_movie    | Movie reference    |
| `date_key`     | INTEGER   | FK ‚Üí dim_date     | Date reference     |
| `rating`       | SMALLINT  | CHECK (1-5)       | Rating value       |
| `created_at`   | TIMESTAMP | DEFAULT NOW()     | Load timestamp     |

**Indexes:**

```sql
PRIMARY KEY (rating_key)
FOREIGN KEY (customer_key) ‚Üí dim_customer(customer_key)
FOREIGN KEY (movie_key) ‚Üí dim_movie(movie_key)
FOREIGN KEY (date_key) ‚Üí dim_date(date_key)
```

#### **Dimension Table: dim_date**

Temporal dimension for analysis by time periods.

| Column                   | Type        | Purpose                         |
| ------------------------ | ----------- | ------------------------------- |
| `date_key`               | INTEGER     | Surrogate key (YYYYMMDD format) |
| `date_actual`            | DATE        | Actual calendar date            |
| `year`, `month`, `day`   | SMALLINT    | Time components                 |
| `quarter`, `day_of_week` | SMALLINT    | Grouping fields                 |
| `month_name`             | VARCHAR(20) | Human-readable month            |
| `is_weekend`             | BOOLEAN     | Weekend flag                    |

**Date Range:** 1998-10-01 to 2005-12-31 (2,865 days)

#### **Dimension Table: dim_movie**

Movie metadata dimension.

| Column         | Type         | Purpose                 |
| -------------- | ------------ | ----------------------- |
| `movie_key`    | SERIAL       | Surrogate key           |
| `movie_id`     | INTEGER      | Natural key from source |
| `title`        | VARCHAR(500) | Movie title             |
| `release_year` | SMALLINT     | Year released           |

**Cardinality:** 17,770 unique movies

#### **Dimension Table: dim_customer**

Customer dimension with aggregated metrics.

| Column              | Type    | Purpose                       |
| ------------------- | ------- | ----------------------------- |
| `customer_key`      | SERIAL  | Surrogate key                 |
| `customer_id`       | INTEGER | Natural key from source       |
| `first_rating_date` | DATE    | Customer's first rating       |
| `last_rating_date`  | DATE    | Customer's most recent rating |
| `total_ratings`     | INTEGER | Count of ratings (aggregate)  |

**Cardinality:** ~480,000 unique customers

### Analytical Queries

#### Top 10 Most-Rated Movies

```sql
SELECT m.title, COUNT(*) as rating_count
FROM fact_ratings fr
JOIN dim_movie m ON fr.movie_key = m.movie_key
GROUP BY m.movie_key, m.title
ORDER BY rating_count DESC
LIMIT 10;
```

#### Average Rating by Year

```sql
SELECT d.year, AVG(fr.rating) as avg_rating
FROM fact_ratings fr
JOIN dim_date d ON fr.date_key = d.date_key
GROUP BY d.year
ORDER BY d.year;
```

#### Customer Rating Trends

```sql
SELECT
    dc.customer_id,
    d.year,
    COUNT(*) as ratings_per_year,
    AVG(fr.rating) as avg_rating
FROM fact_ratings fr
JOIN dim_customer dc ON fr.customer_key = dc.customer_key
JOIN dim_date d ON fr.date_key = d.date_key
GROUP BY dc.customer_id, d.year
ORDER BY dc.customer_id, d.year;
```

---

## üîß ETL Pipeline Details

### Stage 1: Date Dimension (2-3 minutes)

- **Input**: Date range configuration (1998-10-01 to 2005-12-31)
- **Processing**: Generate date sequence with all temporal attributes
- **Output**: 2,865 date records with year, month, quarter, day-of-week
- **Key Features**: Surrogate keys in YYYYMMDD format, weekend flags

### Stage 2: Movie Dimension (1-2 minutes)

- **Input**: `data/movie_titles.csv` (17,770 records)
- **Processing**: Parse CSV with comma-separated titles, assign surrogate keys
- **Output**: Movie dimension with natural and surrogate keys
- **Handling**: Titles containing commas handled correctly via split limit

### Stage 3: Customer Dimension (3-5 minutes)

- **Input**: All combined data files for customer extraction
- **Processing**: Extract unique customers, deduplicate, calculate aggregates
- **Output**: Customer dimension with first/last rating dates and total counts
- **Optimization**: Broadcast to fact table stage for efficient joins

### Stage 4: Fact Table (8-12 minutes)

- **Input**: `data/combined_data_*.txt` (100M+ records)
- **Processing**: Parse movie:rating:date format, join with dimensions, validate
- **Output**: 100M+ fact records with surrogate keys
- **Batch Writing**: 10,000-record JDBC batches for optimal database throughput

### Stage 5: Post-Processing (1-2 minutes)

- **Customer Aggregates**: Update total_ratings and date ranges
- **Index Optimization**: Analyze and optimize query execution plans
- **Validation**: Row count verification across all tables

---

## üêõ Troubleshooting

### Common Issues & Solutions

#### Issue: "PGHOST environment variable not found"

**Cause**: `.env` file missing or incomplete

**Solution**:

```bash
# Copy template and edit
cp .env.example .env
# Edit .env with your PostgreSQL credentials
```

#### Issue: "PostgreSQL JDBC driver not found"

**Cause**: `postgresql-42.6.0.jar` missing from project root

**Solution**:

```bash
# Download from https://jdbc.postgresql.org/download.html
# Place in project root directory
```

#### Issue: "Connection refused to database server"

**Cause**: Database server unreachable or credentials invalid

**Diagnosis**:

```bash
# Test connectivity
telnet your-server.postgres.database.azure.com 5432

# Verify credentials
psql -h your-server -U adminuser -d postgres -c "SELECT 1;"
```

#### Issue: "java.lang.NoSuchMethodError" or Spark errors

**Cause**: Incompatible Spark/Scala/Java versions

**Solution**:

```bash
# Clean and reinstall Spark
pip uninstall pyspark -y
pip install pyspark==3.4.0
```

#### Issue: Out of Memory during Spark execution

**Cause**: Too many partitions or insufficient driver memory

**Solution**:

```bash
# Reduce shuffle partitions in etl_pipeline_spark.py
SHUFFLE_PARTITIONS = 100  # From 200
DEFAULT_PARALLELISM = 100
```

#### Issue: Slow execution or high CPU usage

**Possible Causes & Solutions**:

- Reduce `SHUFFLE_PARTITIONS` if CPU at 100%
- Increase if partitions are underutilized
- Check PostgreSQL connection pool limits
- Monitor disk I/O during data reads

### Log Analysis

**View recent errors**:

```bash
Get-Content etl_pipeline_spark.log | Select-String "ERROR"
```

**View specific stage**:

```bash
Get-Content etl_pipeline_spark.log | Select-String "STAGE 3"
```

**Full execution timeline**:

```bash
Get-Content etl_pipeline_spark.log
```

### Performance Tuning

| Parameter             | Current | For Large Data | For Small Data |
| --------------------- | ------- | -------------- | -------------- |
| `SHUFFLE_PARTITIONS`  | 200     | 400-500        | 50-100         |
| `JDBC_BATCH_SIZE`     | 10,000  | 50,000         | 1,000          |
| `JDBC_NUM_PARTITIONS` | 20      | 100            | 5              |

---

## üìä Performance Benchmarks

Typical execution times on 4-core machine with SSD:

| Stage              | Time        | Records   |
| ------------------ | ----------- | --------- |
| Date Dimension     | 2m          | 2,865     |
| Movie Dimension    | 1.5m        | 17,770    |
| Customer Dimension | 4m          | 480K      |
| Fact Table         | 10m         | 100M+     |
| Post-Processing    | 2m          | -         |
| **Total**          | **~19-20m** | **100M+** |

---

## üîê Security Best Practices

1. **Never commit `.env`** - It's in `.gitignore` for security
2. **Use strong passwords** - Minimum 12 characters with special characters
3. **Enable SSL** - PostgreSQL connection uses `sslmode=require`
4. **Restrict database access** - Use firewall rules on Azure
5. **Rotate credentials** - Regularly update database passwords
6. **Monitor audit logs** - Check Azure Portal for connection attempts

---

## üìù Technical Stack

| Component         | Version | Purpose                    |
| ----------------- | ------- | -------------------------- |
| **Python**        | 3.9+    | Main programming language  |
| **PySpark**       | 3.4.0+  | Distributed ETL processing |
| **PostgreSQL**    | 12+     | Data warehouse             |
| **Pandas**        | 2.0+    | Data validation            |
| **SQLAlchemy**    | 2.0+    | Schema reflection          |
| **python-dotenv** | 1.0+    | Configuration management   |
| **JDBC Driver**   | 42.6.0  | Database connectivity      |

---

## üìö References

### Netflix Prize Dataset

- [Official Dataset Documentation](https://www.kaggle.com/netflix-inc/netflix-prize-data)
- Format: Customer-Movie-Rating-Date tuples

### Star Schema Design

- [Dimensional Modeling](https://en.wikipedia.org/wiki/Dimensional_modeling)
- Optimized for OLAP queries and business intelligence

### Apache Spark Documentation

- [PySpark Official Docs](https://spark.apache.org/docs/latest/api/python/)
- [Spark SQL Guide](https://spark.apache.org/docs/latest/sql-programming-guide.html)
- [Performance Tuning](https://spark.apache.org/docs/latest/tuning.html)

### PostgreSQL

- [Azure Database for PostgreSQL](https://docs.microsoft.com/azure/postgresql/)
- [PostgreSQL Documentation](https://www.postgresql.org/docs/)

---

## üìÑ License

This project is provided as-is for educational and data warehouse purposes.

---

## ‚úâÔ∏è Support

For issues or questions:

1. Check the **Troubleshooting** section above
2. Review `etl_pipeline_spark.log` for error details
3. Verify `.env` configuration
4. Check database connectivity

---

**Last Updated**: December 15, 2025  
**Version**: 2.0 (PySpark Refactor)  
**Status**: Production Ready

- Prerequisites and dependencies
- Step-by-step setup instructions
- Expected runtime benchmarks
- Common pitfalls & solutions
- Verification queries
- Sample analytical queries
- Troubleshooting commands

### 5Ô∏è‚É£ **Supporting Files**

- `.env.example` - Template for database credentials
- `requirements.txt` - Python dependencies
- `.gitignore` - Version control exclusions

---

## üéØ Star Schema Summary

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   dim_date      ‚îÇ (2,650 rows)
‚îÇ date_key (PK)   ‚îÇ
‚îÇ year, month...  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ dim_customer    ‚îÇ      ‚îÇ  fact_ratings   ‚îÇ      ‚îÇ   dim_movie     ‚îÇ
‚îÇ customer_key(PK)‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ rating_key (PK) ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ movie_key (PK)  ‚îÇ
‚îÇ customer_id     ‚îÇ      ‚îÇ customer_key(FK)‚îÇ      ‚îÇ movie_id        ‚îÇ
‚îÇ ~480K rows      ‚îÇ      ‚îÇ movie_key (FK)  ‚îÇ      ‚îÇ title           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ date_key (FK)   ‚îÇ      ‚îÇ ~17K rows       ‚îÇ
                         ‚îÇ rating (1-5)    ‚îÇ      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ ~100M rows      ‚îÇ
                         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Key Design Decisions

‚úÖ **Surrogate Keys**: All dimensions use auto-increment surrogate keys  
‚úÖ **Date Dimension**: Proper time dimension instead of raw dates  
‚úÖ **Referential Integrity**: FK constraints enforced  
‚úÖ **Denormalization**: Optimized for analytical queries  
‚úÖ **Strategic Indexing**: Multi-column indexes for common patterns  
‚úÖ **Type Optimization**: SMALLINT for ratings, BIGSERIAL for fact PK

---

## üöÄ Quick Start

### 1. Install Dependencies

```bash
pip install -r requirements.txt
```

### 2. Configure Database

```bash
cp .env.example .env
# Edit .env with your Azure PostgreSQL credentials
```

### 3. Run ETL Pipeline

```bash
python etl_pipeline.py
```

**Expected Duration**: 2.5-4.5 hours (100M rows)

---

## üìä What Gets Created

| Object                   | Type  | Rows         | Description                |
| ------------------------ | ----- | ------------ | -------------------------- |
| `dim_date`               | Table | ~2,650       | Date dimension (1998-2005) |
| `dim_movie`              | Table | 17,770       | Movie dimension            |
| `dim_customer`           | Table | ~480,189     | Customer dimension         |
| `fact_ratings`           | Table | ~100,480,507 | Rating events (FACT)       |
| `v_daily_rating_summary` | View  | -            | Daily aggregates           |
| `v_movie_performance`    | View  | -            | Movie-level metrics        |

---

## üéì Star Schema Best Practices Applied

1. ‚úÖ **Grain Definition**: One fact row = one rating event
2. ‚úÖ **Conformed Dimensions**: Shared date dimension for temporal analysis
3. ‚úÖ **Surrogate Keys**: Decoupled from natural keys
4. ‚úÖ **Slowly Changing Dimensions**: Type 1 SCD (current state only)
5. ‚úÖ **Fact Table Optimization**: Only FKs and measures
6. ‚úÖ **Query Performance**: Denormalized dimensions
7. ‚úÖ **Scalability**: Handles 100M+ rows efficiently
8. ‚úÖ **BI Tool Ready**: Standard star schema pattern

---

## üîç Verification Checklist

After ETL completion, verify:

- [ ] **Row Counts Match**:

  - dim_date: 2,650 ‚úì
  - dim_movie: 17,770 ‚úì
  - dim_customer: ~480,189 ‚úì
  - fact_ratings: ~100,480,507 ‚úì

- [ ] **No Orphaned Records**: All FKs resolve

- [ ] **Data Quality**:

  - Ratings are 1-5 (integer)
  - Dates within 1998-2005
  - No NULL surrogate keys

- [ ] **Customer Aggregates Updated**:

  - first_rating_date populated
  - total_ratings > 0

- [ ] **Indexes Created**: Check with `\d+ fact_ratings` in psql

---

## üí° Use Cases Enabled

### 1. **Collaborative Filtering (ML)**

```python
# User-item matrix for recommendation systems
query = """
SELECT customer_key, movie_key, rating
FROM netflix_dw.fact_ratings
"""
```

### 2. **Trend Analysis**

```sql
-- Rating volume over time
SELECT d.year, d.month, COUNT(*) as ratings
FROM netflix_dw.fact_ratings f
JOIN netflix_dw.dim_date d ON f.date_key = d.date_key
GROUP BY d.year, d.month
ORDER BY d.year, d.month;
```

### 3. **Movie Recommendations**

```sql
-- Similar customers (who rated the same movies highly)
-- Top-rated movies by genre/year
-- Cold-start problem analysis
```

### 4. **Business Intelligence**

- Tableau/Power BI dashboards
- Customer segmentation
- Content performance analysis
- Temporal rating patterns

---

## ‚öôÔ∏è Technical Specifications

### Database

- **Platform**: Azure Database for PostgreSQL (Flexible Server)
- **Schema**: `netflix_dw`
- **Total Size**: ~15-20 GB (including indexes)
- **Performance**: Optimized for OLAP queries

### Python Requirements

- **Version**: Python 3.9+
- **Key Libraries**: pandas, SQLAlchemy, psycopg2-binary, python-dotenv
- **Memory**: 4GB+ RAM recommended
- **Processing**: Single-threaded (can be parallelized)

### ETL Characteristics

- **Idempotent**: Safe to re-run
- **Chunked Processing**: 50K rows per batch
- **Error Handling**: Graceful skips for malformed data
- **Logging**: Detailed progress tracking
- **Resumability**: Can restart from schema creation

---

## üìà Performance Metrics

### ETL Pipeline

| Stage              | Duration   | Throughput          |
| ------------------ | ---------- | ------------------- |
| Date Dimension     | ~2 sec     | 1,325 rows/sec      |
| Movie Dimension    | ~5 sec     | 3,554 rows/sec      |
| Customer Dimension | ~5 min     | 1,600 rows/sec      |
| **Fact Table**     | **~3 hrs** | **~9,300 rows/sec** |
| Post-Processing    | ~10 min    | -                   |

### Query Performance (Post-ANALYZE)

- Simple aggregations: <1 second
- Complex JOINs (3 tables): 1-5 seconds
- Full table scans: 10-30 seconds

**Note**: Performance depends on Azure tier (vCores, memory, IOPS)

---

## üõ°Ô∏è Data Governance

### Security

- ‚úÖ Credentials via `.env` (not hardcoded)
- ‚úÖ `.gitignore` prevents credential leaks
- ‚úÖ Azure SSL/TLS encryption supported
- ‚úÖ Role-based access control (configure in Azure)

### Data Quality

- ‚úÖ CHECK constraints on ratings (1-5)
- ‚úÖ Foreign key integrity enforced
- ‚úÖ Unique constraints on natural keys
- ‚úÖ NOT NULL on critical fields

### Audit Trail

- ‚úÖ `created_at` timestamp on all tables
- ‚úÖ `rating_timestamp` preserved in fact table
- ‚úÖ ETL logs with timestamps

---

## üîÑ Maintenance

### Regular Tasks

1. **VACUUM ANALYZE** (weekly):

   ```sql
   VACUUM ANALYZE netflix_dw.fact_ratings;
   ```

2. **Index Maintenance** (monthly):

   ```sql
   REINDEX TABLE netflix_dw.fact_ratings;
   ```

3. **Backup** (daily):
   - Use Azure automated backups
   - Or: `pg_dump` for point-in-time snapshots

### Monitoring

- Track table sizes: `pg_total_relation_size()`
- Monitor query performance: `pg_stat_statements`
- Check index usage: `pg_stat_user_indexes`

---

## üìû Next Steps

### Immediate Actions

1. ‚úÖ Review `STAR_SCHEMA_DESIGN.md`
2. ‚úÖ Configure `.env` file
3. ‚úÖ Run `python etl_pipeline.py`
4. ‚úÖ Verify data with queries in `SETUP_GUIDE.md`

### Future Enhancements

- [ ] Add movie genre dimension (from external data)
- [ ] Implement Type 2 SCD for customer evolution
- [ ] Create materialized views for popular aggregations
- [ ] Partition fact table by date for better performance
- [ ] Implement incremental loads (CDC)
- [ ] Add data quality monitoring
- [ ] Create Tableau/Power BI templates

---

## üéâ Success Criteria

You'll know it worked when:

‚úÖ All tables created with correct row counts  
‚úÖ No referential integrity violations  
‚úÖ Sample analytical queries return results in <5 seconds  
‚úÖ Customer aggregates populated correctly  
‚úÖ `etl_pipeline.log` shows "COMPLETED SUCCESSFULLY"  
‚úÖ You can run ML models on the user-item rating matrix

---

## üìö Additional Resources

- **Dataset**: [Netflix Prize on Academic Torrents](http://academictorrents.com/details/9b13183dc4d60676b773c9e2cd6de5e5542cee9a)
- **Star Schema**: [Kimball Group - Data Warehouse Toolkit](https://www.kimballgroup.com/)
- **PostgreSQL**: [Official Documentation](https://www.postgresql.org/docs/)
- **Azure PostgreSQL**: [Microsoft Docs](https://docs.microsoft.com/en-us/azure/postgresql/)

---

**Project Status**: ‚úÖ Production Ready  
**Version**: 1.0  
**Last Updated**: December 14, 2025

**Questions?** Check `SETUP_GUIDE.md` for troubleshooting.
